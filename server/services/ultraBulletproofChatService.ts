import { z } from 'zod';

// Ultra simple schema - accepts almost anything
const ultraSimpleSchema = z.object({
  message: z.string().min(1).default('Hello'),
  sessionId: z.string().optional(),
  model: z.string().optional().default('openai/gpt-4o'),
  userId: z.string().optional().default('demo-user'),
  stream: z.boolean().optional().default(false)
});

type UltraSimpleRequest = z.infer<typeof ultraSimpleSchema>;

// Streaming response type for SSE (Server-Sent Events)
type StreamingResponse = {
  success: boolean;
  sessionId: string;
  model: string;
  source: string;
  stream: ReadableStream<Uint8Array>;
  metadata: any;
};

// Response cache for lightning-fast responses
const responseCache = new Map<string, { response: string; timestamp: number }>();
const CACHE_TTL = 300000; // 5 minute cache for speed

class UltraBulletproofChatService {
  private attemptCounter = 0;

  // STREAMING: Direct streaming response following OpenRouter SSE docs
  async processUltraBulletproofStream(request: any): Promise<StreamingResponse> {
    const startTime = Date.now();
    this.attemptCounter++;
    
    // Step 1: Validate input with extreme tolerance
    let validatedRequest: UltraSimpleRequest;
    try {
      validatedRequest = ultraSimpleSchema.parse(request);
    } catch (error) {
      validatedRequest = {
        message: String(request?.message || request?.query || request?.text || 'Hello'),
        sessionId: String(request?.sessionId || `ultra-${Date.now()}`),
        model: String(request?.model || 'openai/gpt-4o'),
        userId: 'demo-user',
        stream: true
      };
    }

    const { message, sessionId = `ultra-${Date.now()}`, model, userId } = validatedRequest;

    console.log(`ðŸš€ STREAMING: OpenRouter call with model ${model}`);
    
    try {
      const streamResponse = await this.fastDirectOpenRouterStream(message, model);
      if (streamResponse) {
        return {
          success: true,
          sessionId,
          model,
          source: `openrouter-stream-${model}`,
          stream: streamResponse,
          metadata: {
            processingTime: Date.now() - startTime,
            attemptNumber: this.attemptCounter,
            streaming: true
          }
        };
      }
    } catch (error) {
      console.warn('Direct OpenRouter stream failed:', error);
    }

    // Fallback to non-streaming if streaming fails
    throw new Error('Streaming failed, use non-streaming endpoint');
  }

  // NUCLEAR FAST: Direct AI response with caching
  async processUltraBulletproofMessage(request: any): Promise<{
    success: boolean;
    content: string;
    sessionId: string;
    model: string;
    source: string;
    metadata: any;
  }> {
    const startTime = Date.now();
    this.attemptCounter++;
    
    // Step 1: Validate input with extreme tolerance
    let validatedRequest: UltraSimpleRequest;
    try {
      validatedRequest = ultraSimpleSchema.parse(request);
    } catch (error) {
      validatedRequest = {
        message: String(request?.message || request?.query || request?.text || 'Hello'),
        sessionId: String(request?.sessionId || `ultra-${Date.now()}`),
        model: String(request?.model || 'openai/gpt-4o'),
        userId: 'demo-user',
        stream: false // Non-streaming method defaults to false
      };
    }

    const { message, sessionId = `ultra-${Date.now()}`, model, userId } = validatedRequest;

    // Step 2: Check cache first for instant responses
    const cacheKey = `${userId}-${model}-${message.substring(0, 100)}`; // Include model in cache key!
    const cached = responseCache.get(cacheKey);
    if (cached && (Date.now() - cached.timestamp) < CACHE_TTL) {
      console.log('âš¡ INSTANT: Cached response');
      return {
        success: true,
        content: cached.response,
        sessionId,
        model,
        source: 'cache-instant',
        metadata: {
          cached: true,
          processingTime: Date.now() - startTime,
          attemptNumber: this.attemptCounter
        }
      };
    }

    // Step 3: NUCLEAR FAST - Direct OpenRouter call with selected model
    console.log(`ðŸš€ NUCLEAR FAST: OpenRouter call with model ${model}`);
    
    try {
      const directResponse = await this.fastDirectOpenRouter(message, model);
      if (directResponse) {
        // Cache for future speed with model-specific key
        responseCache.set(cacheKey, {
          response: directResponse,
          timestamp: Date.now()
        });
        
        return {
          success: true,
          content: directResponse,
          sessionId,
          model,
          source: `openrouter-${model}`,
          metadata: {
            processingTime: Date.now() - startTime,
            attemptNumber: this.attemptCounter
          }
        };
      }
    } catch (error) {
      console.warn('Direct OpenAI failed:', error);
    }

    // Step 4: Intelligent fallback (guaranteed response)
    const intelligentFallback = this.generateSmartFallback(message);
    
    // Cache even fallback for consistency with model-specific key
    responseCache.set(cacheKey, {
      response: intelligentFallback,
      timestamp: Date.now()
    });

    // ALWAYS return a response - ultimate guarantee
    return {
      success: true,
      content: intelligentFallback,
      sessionId,
      model,
      source: 'smart-fallback',
      metadata: {
        processingTime: Date.now() - startTime,
        fallback: true,
        attemptNumber: this.attemptCounter
      }
    };
  }

  // Fast Direct OpenRouter call - respects model selection
  private async fastDirectOpenRouter(message: string, model: string): Promise<string | null> {
    const API_KEY = process.env.OPENROUTER_API_KEY2 || process.env.OPENAI_API_KEY;
    if (!API_KEY) {
      console.warn('No OpenRouter API key found');
      return null;
    }

    // VERIFIED WORKING MODELS ONLY - matches /api/openrouter-models endpoint
    const ELITE_MODELS = new Set([
      // OpenAI models (working with OpenRouter)
      "openai/gpt-4o",
      "openai/gpt-4o-mini", 
      "openai/gpt-4-turbo",
      "openai/gpt-4",
      "openai/chatgpt-4o-latest",
      
      // Anthropic models (working)
      "anthropic/claude-3.5-sonnet",
      "anthropic/claude-3.5-haiku",
      "anthropic/claude-3-opus",
      "anthropic/claude-3-haiku",
      
      // xAI models (REAL models, not phantom grok-4)
      "x-ai/grok-2-1212",
      "x-ai/grok-2-vision-1212",
      "x-ai/grok-vision-beta",
      
      // Perplexity models (working)
      "perplexity/sonar-deep-research",
      "perplexity/sonar-reasoning-pro",
      "perplexity/sonar-pro",
      "perplexity/sonar-reasoning",
      "perplexity/sonar"
    ]);

    // Use the selected model if it's in the allowlist, otherwise fallback
    const finalModel = ELITE_MODELS.has(model) ? model : 'openai/gpt-4o';
    
    console.log(`ðŸŽ¯ Using model: ${finalModel}`);

    try {
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${API_KEY}`,
          'HTTP-Referer': process.env.SITE_URL || 'http://localhost:5000',
          'X-Title': 'Bristol Development AI'
        },
        body: JSON.stringify({
          model: finalModel, // USE THE SELECTED MODEL!
          messages: [
            {
              role: 'system',
              content: 'You are Bristol A.I. Elite, Bristol Development Group\'s institutional real estate AI. Provide concise, professional responses focused on multifamily development analysis, market insights, and financial modeling.'
            },
            {
              role: 'user',
              content: message
            }
          ],
          temperature: 0.3,
          max_tokens: 800
          // Non-streaming version - stream: false is default
        })
      });

      if (response.ok) {
        const data = await response.json();
        console.log(`âœ… OpenRouter response received from model: ${finalModel}`);
        return data.choices?.[0]?.message?.content || null;
      } else {
        const errorText = await response.text();
        console.error(`OpenRouter API error (${response.status}):`, errorText);
      }
    } catch (error) {
      console.error('Fast OpenRouter error:', error);
    }
    return null;
  }

  // Fast Direct OpenRouter STREAMING call - follows OpenRouter Streaming docs exactly
  private async fastDirectOpenRouterStream(message: string, model: string): Promise<ReadableStream<Uint8Array> | null> {
    const API_KEY = process.env.OPENROUTER_API_KEY2 || process.env.OPENAI_API_KEY;
    if (!API_KEY) {
      console.warn('No OpenRouter API key found');
      return null;
    }

    // VERIFIED WORKING MODELS ONLY - same allowlist as non-streaming
    const ELITE_MODELS = new Set([
      "openai/gpt-4o", "openai/gpt-4o-mini", "openai/gpt-4-turbo", "openai/gpt-4", "openai/chatgpt-4o-latest",
      "anthropic/claude-3.5-sonnet", "anthropic/claude-3.5-haiku", "anthropic/claude-3-opus", "anthropic/claude-3-haiku",
      "x-ai/grok-2-1212", "x-ai/grok-2-vision-1212", "x-ai/grok-vision-beta",
      "perplexity/sonar-deep-research", "perplexity/sonar-reasoning-pro", "perplexity/sonar-pro", "perplexity/sonar-reasoning", "perplexity/sonar"
    ]);

    const finalModel = ELITE_MODELS.has(model) ? model : 'openai/gpt-4o';
    console.log(`ðŸŽ¯ Streaming model: ${finalModel}`);

    try {
      // Following OpenRouter Streaming docs EXACTLY - TypeScript example
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${API_KEY}`,
          'Content-Type': 'application/json',
          'HTTP-Referer': process.env.SITE_URL || 'http://localhost:5000',
          'X-Title': 'Bristol Development AI'
        },
        body: JSON.stringify({
          model: finalModel,
          messages: [
            {
              role: 'system',
              content: 'You are Bristol A.I. Elite, Bristol Development Group\'s institutional real estate AI. Provide concise, professional responses focused on multifamily development analysis, market insights, and financial modeling.'
            },
            {
              role: 'user',
              content: message
            }
          ],
          stream: true, // STREAMING ENABLED per OpenRouter docs
          temperature: 0.3,
          max_tokens: 800
        })
      });

      if (response.ok && response.body) {
        console.log(`âœ… OpenRouter streaming response from model: ${finalModel}`);
        return response.body; // Return the ReadableStream directly
      } else {
        const errorText = await response.text();
        console.error(`OpenRouter streaming API error (${response.status}):`, errorText);
      }
    } catch (error) {
      console.error('Fast OpenRouter streaming error:', error);
    }
    return null;
  }

  // Smart fallback responses based on message context
  private generateSmartFallback(message: string): string {
    const lowerMessage = message.toLowerCase();
    
    if (lowerMessage.includes('property') || lowerMessage.includes('real estate')) {
      return `I understand you're asking about property analysis regarding: "${message.substring(0, 80)}...". As Bristol A.I. Elite, I analyze multifamily properties using our institutional-grade methodology. Key factors include location scoring, demographic trends, cap rates (targeting 5-7%), cash flow projections, and market comparables. What specific property metrics would you like me to evaluate?`;
    }
    
    if (lowerMessage.includes('market') || lowerMessage.includes('analysis')) {
      return `You're inquiring about market analysis for: "${message.substring(0, 80)}...". Bristol Development Group specializes in Sunbelt market intelligence. I evaluate population growth rates, employment trends, housing supply/demand, and demographic shifts. Our target markets show 2%+ annual population growth. Which market metrics are most important for your analysis?`;
    }
    
    if (lowerMessage.includes('irr') || lowerMessage.includes('npv') || lowerMessage.includes('financial')) {
      return `I see you're interested in financial modeling related to: "${message.substring(0, 80)}...". For multifamily value-add projects, we target IRRs of 15-20% and stress-test scenarios at 80% occupancy. NPV calculations include acquisition costs, renovation expenses, and exit strategies. What financial assumptions should I model for your analysis?`;
    }
    
    return `I received your message: "${message.substring(0, 100)}${message.length > 100 ? '...' : ''}". As Bristol A.I. Elite, I'm your institutional-grade real estate AI assistant. I specialize in multifamily development analysis, market intelligence, and financial modeling with 30+ years of Bristol expertise. How can I assist with your real estate investment needs today?`;
  }

  // Clear cache for admin use
  clearCache(): void {
    responseCache.clear();
    this.attemptCounter = 0;
  }

  // Get service stats
  getStats(): any {
    return {
      cacheSize: responseCache.size,
      attemptCounter: this.attemptCounter,
      cacheKeys: Array.from(responseCache.keys()).slice(0, 5)
    };
  }
}

export const ultraBulletproofChatService = new UltraBulletproofChatService();