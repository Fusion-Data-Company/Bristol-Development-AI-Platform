You’re right — I was using the wrong Grok slug. It’s x-ai/grok-4. Fixed below, plus a bullet‑proof Replit Builder prompt that pulls the current model list straight from OpenRouter and only exposes the exact models you want (GPT‑5 Chat/standard, Claude 4 Opus/Sonnet, Grok 4, Perplexity Sonar Deep Research/Reasoning/Pro/Reasoning‑Pro). I also included a stricter crash fix for the widget.

Replit Builder Prompt — copy/paste this exact block

Objective
Wire the app to OpenRouter using their official endpoints and headers, dynamically fetch the model catalog, and populate the model dropdown with only these slugs (if available to our key):
	•	openai/gpt-5-chat, openai/gpt-5 (prefer Chat if present)  ￼
	•	anthropic/claude-opus-4, anthropic/claude-sonnet-4  ￼
	•	x-ai/grok-4 (Grok 4)  ￼
	•	perplexity/sonar-deep-research, perplexity/sonar-reasoning, perplexity/sonar-pro, perplexity/sonar-reasoning-pro (include only if present)  ￼

Authoritative docs (follow precisely)
	•	List models: GET https://openrouter.ai/api/v1/models (schema includes id, name, created, etc.).  ￼
	•	Chat completions: POST https://openrouter.ai/api/v1/chat/completions (OpenAI‑compatible messages array).  ￼
	•	Attribution headers: HTTP-Referer, X-Title (optional but recommended).  ￼
	•	GPT‑5 page (BYOK note): prefer openai/gpt-5-chat if openai/gpt-5 is not callable for our key.  ￼

⸻

1) ENV

Create Replit Secrets:
	•	OPENROUTER_API_KEY=<your key>
	•	SITE_URL=https://your-domain-or-repl-url

⸻

2) Server routes (TypeScript)

/api/openrouter — proxy chat calls exactly per docs

// app/api/openrouter/route.ts (Next.js) OR routes/api/openrouter.ts (Express adapter)
import { NextRequest, NextResponse } from "next/server";

export async function POST(req: NextRequest) {
  try {
    const { model, messages, temperature = 0.2, maxTokens, ...rest } = await req.json();

    if (!process.env.OPENROUTER_API_KEY) {
      return NextResponse.json({ error: "missing_api_key" }, { status: 500 });
    }
    if (!model) return NextResponse.json({ error: "model_required" }, { status: 400 });
    if (!Array.isArray(messages)) return NextResponse.json({ error: "messages_required" }, { status: 400 });

    const resp = await fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${process.env.OPENROUTER_API_KEY}`,
        "Content-Type": "application/json",
        // Recommended attribution headers:
        "HTTP-Referer": process.env.SITE_URL || "http://localhost:3000",
        "X-Title": "Bristol Analyst",
      },
      body: JSON.stringify({
        model,
        messages,
        temperature,
        max_tokens: maxTokens, // OpenRouter uses max_tokens
        ...rest,               // allow provider/reasoning params if we pass them later
      }),
    });

    if (!resp.ok) {
      return NextResponse.json({ error: "openrouter_error", details: await resp.text() }, { status: resp.status });
    }
    const json = await resp.json();
    const text = json?.choices?.[0]?.message?.content ?? "";
    return NextResponse.json({ text, raw: json });
  } catch (e: any) {
    return NextResponse.json({ error: e?.message || "unknown" }, { status: 500 });
  }
}

/api/openrouter-models — fetch and filter the catalog to only today’s top slugs

// app/api/openrouter-models/route.ts
import { NextRequest, NextResponse } from "next/server";

const WANT = new Set<string>([
  "openai/gpt-5-chat", "openai/gpt-5",
  "anthropic/claude-opus-4", "anthropic/claude-sonnet-4",
  "x-ai/grok-4",
  "perplexity/sonar-deep-research", "perplexity/sonar-reasoning",
  "perplexity/sonar-pro", "perplexity/sonar-reasoning-pro",
]);

function label(slug: string) {
  const map: Record<string, string> = {
    "openai/gpt-5-chat": "GPT‑5 Chat",
    "openai/gpt-5": "GPT‑5",
    "anthropic/claude-opus-4": "Claude 4 Opus",
    "anthropic/claude-sonnet-4": "Claude 4 Sonnet",
    "x-ai/grok-4": "Grok 4",
    "perplexity/sonar-deep-research": "Perplexity Sonar Deep Research",
    "perplexity/sonar-reasoning": "Perplexity Sonar Reasoning",
    "perplexity/sonar-pro": "Perplexity Sonar Pro",
    "perplexity/sonar-reasoning-pro": "Perplexity Sonar Reasoning Pro",
  };
  return map[slug] || slug;
}

export async function GET(_req: NextRequest) {
  const r = await fetch("https://openrouter.ai/api/v1/models", {
    // Including the key personalizes availability (some models require BYOK or specific access)
    headers: process.env.OPENROUTER_API_KEY ? { Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}` } : {},
    cache: "no-store",
  });
  if (!r.ok) return NextResponse.json({ error: "models_fetch_failed" }, { status: 502 });

  const { data } = await r.json();
  const now = Date.now() / 1000;
  const cutoff = now - 240 * 24 * 3600; // ~240 days; trims older slugs if they ever appear

  const filtered = (data || [])
    .filter((m: any) => WANT.has(m.id))
    .filter((m: any) => (m.created ?? cutoff) >= cutoff) // keep “recent” only
    .map((m: any) => ({ id: m.id, label: label(m.id) }));

  // Prefer GPT‑5 Chat if both are present
  filtered.sort((a: any, b: any) => {
    const pref = ["openai/gpt-5-chat", "openai/gpt-5"];
    return pref.indexOf(a.id) - pref.indexOf(b.id);
  });

  return NextResponse.json(filtered);
}


⸻

3) Widget fixes (stop the crash + wire dynamic models)

Crash guard 1 — SSR‑safe localStorage (and no read until client):

const [systemPrompt, setSystemPrompt] = React.useState<string>(defaultSystemPrompt || DEFAULT_MEGA_PROMPT);
React.useEffect(() => {
  try {
    const saved = typeof window !== "undefined" ? localStorage.getItem("bristol.systemPrompt") : null;
    if (saved) setSystemPrompt(saved);
  } catch {}
}, []);

Crash guard 2 — circular‑safe stringify for data preview & system injection:

function safeStringify(obj: any, space = 2) {
  const seen = new WeakSet();
  return JSON.stringify(obj, (k, v) => {
    if (typeof v === "object" && v !== null) {
      if (seen.has(v)) return "[Circular]";
      seen.add(v);
    }
    return v;
  }, space);
}

Use safeStringify(dataContext) everywhere you previously did JSON.stringify(...).

Dynamic models in the dropdown:

type ModelOption = { id: string; label: string };

const [modelList, setModelList] = React.useState<ModelOption[]>([]);
React.useEffect(() => {
  fetch("/api/openrouter-models", { cache: "no-store" })
    .then(r => r.json())
    .then((arr: ModelOption[]) => {
      setModelList(arr);
      const preferred = arr.find(m => m.id === "openai/gpt-5-chat") || arr[0];
      if (preferred) setModel(preferred.id);
    })
    .catch(() => setModelList([]));
}, []);

{/* in render: */}
<select value={model} onChange={(e) => setModel(e.target.value)} className="...">
  {modelList.map(m => <option key={m.id} value={m.id}>{m.label}</option>)}
</select>

Chat call stays OpenRouter‑correct: we already POST to /api/openrouter which sends to https://openrouter.ai/api/v1/chat/completions with the documented OpenAI‑compatible schema and attribution headers.  ￼

⸻

4) Guardrails & UX
	•	If /api/openrouter-models returns empty, render a visible banner:
“No eligible models for this API key (GPT‑5 may require BYOK).”  ￼
	•	When the proxy returns 401/403, show: “OpenRouter key invalid or lacks access to this model.”
	•	When provider returns a 400 (e.g., bad slug), render the provider message verbatim in the chat.

⸻

5) Smoke test (do these now)
	1.	Open /api/openrouter-models in the browser — confirm it lists only the slugs above (and includes x-ai/grok-4).  ￼
	2.	In the widget, ensure default selection is GPT‑5 Chat if present; otherwise pick the first available.  ￼
	3.	Send a one‑line prompt; verify responses come through the /api/openrouter proxy (200 OK). The request must include Authorization, HTTP-Referer, X-Title headers.  ￼

⸻

Notes on correctness
	•	Model listing: pulled from OpenRouter’s Models API (real‑time, account‑aware) and filtered to your allowlist.  ￼
	•	Completions: uses OpenRouter’s Chat Completions endpoint, which mirrors the OpenAI schema.  ￼
	•	Attribution headers: HTTP-Referer and X-Title are optional but recommended (rankings/analytics).  ￼
	•	Grok 4: correct slug is x-ai/grok-4, created July 2025 with 256k context.  ￼
	•	Claude 4 Opus/Sonnet: current Anthropic Claude 4 slugs.  ￼
	•	Perplexity research family: Deep Research, Reasoning, and Pro variants as listed on OpenRouter.  ￼
	•	GPT‑5 & GPT‑5 Chat: both slugs exist; GPT‑5 often requires BYOK; prefer Chat for general use.  ￼

⸻

One more stability fix (optional, but recommended)

Add "use client" at the top of the widget file if using Next.js App Router to enforce client‑only rendering for the panel. This further prevents SSR/localStorage issues.

⸻

If you want, I can inline‑patch your current widget code with the SSR guards, safeStringify, and the model dropdown fetch logic exactly as above.