import React, { useEffect, useMemo, useRef, useState } from "react";
import { X, PanelLeftOpen, Send, Settings, Database, MessageSquare } from "lucide-react";

/**
 * BristolFloatingWidget.tsx — v1.0
 * Enterprise-grade floating analyst widget for Bristol Development.
 *
 * WHAT IT DOES
 * - Slides out from the LEFT edge as a floating widget.
 * - Conversationally analyzes ANY in-app data (API responses + DB objects) passed in via props or a global bus.
 * - Model switcher (OpenRouter.io) with per-thread system prompt.
 * - Admin tab to edit/view the Bristol "mega prompt" (stored locally or via callbacks).
 * - Data tab to inspect the merged data that the agent can reason over.
 *
 * HOW TO WIRE IT (quick):
 * 1) Place <BristolFloatingWidget appData={yourMergedData} /> high in app tree.
 * 2) Implement a server proxy /api/openrouter (Node/Edge) to call OpenRouter with your key (never ship the key to the browser).
 * 3) Optional: stream tool usage + telemetry to n8n via webhookUrl.
 * 4) Provide onSaveSystemPrompt/onSend handlers if you want to persist prompts/messages elsewhere.
 *
 * SECURITY
 * - This component NEVER calls OpenRouter directly with an API key. It POSTs to /api/openrouter.
 * - Make sure your proxy validates model names and rate-limits.
 */

// ---------- Types ----------
export type BristolWidgetProps = {
  appData?: Record<string, any>; // Any merged API/DB state from the app
  defaultSystemPrompt?: string; // Optional initial system prompt
  defaultModel?: string; // e.g., "openrouter/gpt-5" (proxy must map to real model)
  webhookUrl?: string; // optional n8n telemetry sink
  onSaveSystemPrompt?: (prompt: string) => Promise<void> | void;
  onSend?: (payload: ChatPayload) => Promise<void> | void; // tap outgoing chat payloads
  className?: string;
};

export type ChatMessage = {
  role: "system" | "user" | "assistant";
  content: string;
  createdAt?: string;
};

export type ChatPayload = {
  model: string;
  messages: ChatMessage[];
  dataContext?: Record<string, any>;
  temperature?: number;
  maxTokens?: number;
};

// ---------- Default Bristol Mega Prompt (short safe baseline) ----------
const DEFAULT_MEGA_PROMPT = `You are the Bristol Development Research Analyst.\n\nMission: Read the provided data context and return concise, defensible answers for high-end real estate due diligence in Middle Tennessee (and beyond).\n\nRules:\n- Be precise with units, ranges, dates, and sources when available.\n- If a metric is missing, say so and suggest the best public source (link names, not raw URLs).\n- Show your working briefly: bullet the key signals and caveats.\n- Never hallucinate addresses or unit counts.\n- If the user asks for actions, return a JSON block labelled ACTIONS with clear next steps.\n`;

// ---------- Common OpenRouter model identifiers (your proxy should map/validate) ----------
const DEFAULT_MODELS = [
  { id: "openrouter/gpt-5", label: "GPT‑5 (OpenRouter)" },
  { id: "anthropic/claude-3.5-sonnet", label: "Claude 3.5 Sonnet" },
  { id: "openai/gpt-4.1", label: "GPT‑4.1" },
  { id: "google/gemini-1.5-pro", label: "Gemini 1.5 Pro" },
];

// ---------- Utilities ----------
const nowISO = () => new Date().toISOString();
const cx = (...classes: (string | undefined | false)[]) => classes.filter(Boolean).join(" ");

// ---------- Component ----------
export default function BristolFloatingWidget({
  appData = {},
  defaultSystemPrompt,
  defaultModel = DEFAULT_MODELS[0].id,
  webhookUrl,
  onSaveSystemPrompt,
  onSend,
  className,
}: BristolWidgetProps) {
  const [open, setOpen] = useState(false);
  const [activeTab, setActiveTab] = useState<"chat" | "data" | "admin">("chat");
  const [model, setModel] = useState(defaultModel);
  const [systemPrompt, setSystemPrompt] = useState<string>(() =>
    localStorage.getItem("bristol.systemPrompt") || defaultSystemPrompt || DEFAULT_MEGA_PROMPT
  );
  const [messages, setMessages] = useState<ChatMessage[]>([
    { role: "system", content: systemPrompt, createdAt: nowISO() },
  ]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);
  const inputRef = useRef<HTMLInputElement | null>(null);

  // Keep the system message in sync if user edits Admin tab
  useEffect(() => {
    setMessages((prev) => {
      const rest = prev.filter((m) => m.role !== "system");
      return [{ role: "system", content: systemPrompt, createdAt: nowISO() }, ...rest];
    });
  }, [systemPrompt]);

  // Memoized merged data so we can show a clean inspector
  const dataContext = useMemo(() => ({
    timestamp: nowISO(),
    appData,
  }), [appData]);

  // Optional telemetry to n8n
  const sendTelemetry = async (event: string, payload: any) => {
    if (!webhookUrl) return;
    try {
      await fetch(webhookUrl, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ event, at: nowISO(), payload }),
        keepalive: true,
      });
    } catch (err) {
      // Silent fail — don’t break UX on telemetry issues
      console.warn("Telemetry failed", err);
    }
  };

  const handleSend = async () => {
    const trimmed = input.trim();
    if (!trimmed || loading) return;
    const userMessage: ChatMessage = { role: "user", content: trimmed, createdAt: nowISO() };
    const newMessages = [...messages, userMessage];
    setMessages(newMessages);
    setInput("");
    inputRef.current?.focus();
    setLoading(true);

    const payload: ChatPayload = {
      model,
      messages: newMessages,
      dataContext, // IMPORTANT: gives the model the live app data
      temperature: 0.2,
      maxTokens: 1200,
    };

    onSend?.(payload);
    sendTelemetry("chat_send", { model, promptSize: JSON.stringify(newMessages).length });

    try {
      // The proxy should call OpenRouter, inject the dataContext into the system or tool context, and stream back tokens
      const res = await fetch("/api/openrouter", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(payload),
      });

      if (!res.ok) throw new Error(`Proxy error ${res.status}`);

      // Supports both JSON and text-stream; here we do simple JSON for clarity
      const data = await res.json();
      const assistantText: string = data?.text ?? data?.message ?? "(No response)";

      const assistantMessage: ChatMessage = {
        role: "assistant",
        content: assistantText,
        createdAt: nowISO(),
      };
      setMessages((prev) => [...prev, assistantMessage]);
      sendTelemetry("chat_receive", { tokens: assistantText.length });
    } catch (err: any) {
      const assistantMessage: ChatMessage = {
        role: "assistant",
        content: `Error: ${err?.message || "Failed to reach model proxy."}`,
        createdAt: nowISO(),
      };
      setMessages((prev) => [...prev, assistantMessage]);
    } finally {
      setLoading(false);
    }
  };

  const saveSystemPrompt = async () => {
    localStorage.setItem("bristol.systemPrompt", systemPrompt);
    await onSaveSystemPrompt?.(systemPrompt);
    sendTelemetry("system_prompt_saved", { size: systemPrompt.length });
  };

  return (
    <>
      {/* Launcher Button (fixed on left) */}
      <button
        onClick={() => setOpen(true)}
        className={cx(
          "fixed top-24 left-0 z-50 group flex items-center gap-2 pl-2 pr-3 py-2 bg-black/80 text-white rounded-r-2xl shadow-xl",
          "backdrop-blur border border-white/10 hover:bg-black"
        )}
        aria-label="Open Bristol Analyst"
      >
        <PanelLeftOpen className="h-5 w-5" />
        <span className="hidden sm:block text-sm font-medium">Bristol Analyst</span>
      </button>

      {/* Slideout Panel */}
      {open && (
        <div className="fixed inset-0 z-50">
          {/* Backdrop */}
          <div className="absolute inset-0 bg-black/40" onClick={() => setOpen(false)} />

          {/* Panel */}
          <div className="absolute inset-y-0 left-0 w-[92vw] sm:w-[560px] bg-neutral-900 text-neutral-100 shadow-2xl border-r border-white/10 flex flex-col">
            <div className="flex items-center justify-between px-4 py-3 border-b border-white/10">
              <div className="flex items-center gap-2">
                <MessageSquare className="h-5 w-5" />
                <span className="font-semibold">Bristol Development — Analyst</span>
              </div>
              <button onClick={() => setOpen(false)} className="p-2 hover:bg-white/5 rounded-lg" aria-label="Close">
                <X className="h-5 w-5" />
              </button>
            </div>

            {/* Controls */}
            <div className="px-4 py-3 flex flex-wrap items-center gap-3 border-b border-white/10">
              <select
                className="bg-neutral-800 border border-white/10 rounded-lg px-3 py-2 text-sm focus:outline-none"
                value={model}
                onChange={(e) => setModel(e.target.value)}
              >
                {DEFAULT_MODELS.map((m) => (
                  <option key={m.id} value={m.id}>{m.label}</option>
                ))}
              </select>

              <div className="ml-auto flex items-center gap-2 text-xs text-neutral-400">
                <span className="hidden sm:block">Context objects:</span>
                <span className="px-2 py-1 rounded bg-white/5 border border-white/10">{Object.keys(appData || {}).length}</span>
              </div>
            </div>

            {/* Tabs */}
            <div className="px-4 pt-3 flex items-center gap-2 text-sm">
              <TabButton icon={<MessageSquare className="h-4 w-4" />} active={activeTab === "chat"} label="Chat" onClick={() => setActiveTab("chat")} />
              <TabButton icon={<Database className="h-4 w-4" />} active={activeTab === "data"} label="Data" onClick={() => setActiveTab("data")} />
              <TabButton icon={<Settings className="h-4 w-4" />} active={activeTab === "admin"} label="Admin" onClick={() => setActiveTab("admin")} />
            </div>

            {/* Body */}
            <div className="flex-1 min-h-0">
              {activeTab === "chat" && <ChatPane messages={messages} loading={loading} />}
              {activeTab === "data" && <DataPane data={dataContext} />}
              {activeTab === "admin" && (
                <AdminPane
                  systemPrompt={systemPrompt}
                  setSystemPrompt={setSystemPrompt}
                  onSave={saveSystemPrompt}
                />
              )}
            </div>

            {/* Composer */}
            {activeTab === "chat" && (
              <div className="border-t border-white/10 p-3 flex items-center gap-2">
                <input
                  ref={inputRef}
                  value={input}
                  onChange={(e) => setInput(e.target.value)}
                  onKeyDown={(e) => e.key === "Enter" && !e.shiftKey ? handleSend() : null}
                  placeholder={loading ? "Waiting for reply…" : "Ask about the current property set, comps, rents, yields, red flags…"}
                  disabled={loading}
                  className="flex-1 bg-neutral-800 border border-white/10 rounded-lg px-3 py-2 text-sm focus:outline-none disabled:opacity-60"
                />
                <button
                  onClick={handleSend}
                  disabled={loading}
                  className="inline-flex items-center gap-2 px-3 py-2 rounded-lg bg-white text-black text-sm font-semibold hover:opacity-90 disabled:opacity-60"
                >
                  <Send className="h-4 w-4" />
                  Send
                </button>
              </div>
            )}
          </div>
        </div>
      )}
    </>
  );
}

// ---------- Subcomponents ----------
function TabButton({ icon, label, active, onClick }: { icon: React.ReactNode; label: string; active?: boolean; onClick: () => void }) {
  return (
    <button
      onClick={onClick}
      className={cx(
        "flex items-center gap-2 px-3 py-2 rounded-lg border text-xs sm:text-sm",
        active ? "bg-white text-black border-white" : "bg-white/5 text-white border-white/10 hover:bg-white/10"
      )}
    >
      {icon}
      {label}
    </button>
  );
}

function ChatPane({ messages, loading }: { messages: ChatMessage[]; loading: boolean }) {
  return (
    <div className="h-full overflow-y-auto px-4 py-3 space-y-3">
      {messages.map((m, i) => (
        <div key={i} className={cx("p-3 rounded-lg border", m.role === "assistant" ? "bg-white/5 border-white/10" : m.role === "system" ? "bg-amber-500/10 border-amber-300/20" : "bg-black/20 border-white/10") }>
          <div className="text-[10px] uppercase tracking-wide opacity-60 mb-1">{m.role}</div>
          <div className="whitespace-pre-wrap text-sm leading-relaxed">{m.content}</div>
        </div>
      ))}
      {loading && (
        <div className="p-3 rounded-lg border bg-white/5 border-white/10 animate-pulse text-sm">Model is thinking…</div>
      )}
    </div>
  );
}

function DataPane({ data }: { data: any }) {
  return (
    <div className="h-full overflow-y-auto p-4">
      <pre className="text-xs leading-relaxed bg-black/40 p-3 rounded-lg border border-white/10 overflow-x-auto">
        {JSON.stringify(data, null, 2)}
      </pre>
      <p className="text-xs text-neutral-400 mt-3">
        Tip: pass merged API/DB state via <code>appData</code>. The widget injects it into each chat request as <code>dataContext</code> for grounded answers.
      </p>
    </div>
  );
}

function AdminPane({ systemPrompt, setSystemPrompt, onSave }: { systemPrompt: string; setSystemPrompt: (v: string) => void; onSave: () => void }) {
  return (
    <div className="h-full flex flex-col">
      <div className="p-3 border-b border-white/10">
        <div className="text-sm font-medium">System Prompt (Bristol Mega Prompt)</div>
        <div className="text-xs text-neutral-400">This is prepended to each thread. Keep it tight and operational.</div>
      </div>
      <div className="p-3 flex-1 min-h-0">
        <textarea
          value={systemPrompt}
          onChange={(e) => setSystemPrompt(e.target.value)}
          className="w-full h-full text-sm bg-neutral-800 border border-white/10 rounded-lg p-3 focus:outline-none"
        />
      </div>
      <div className="p-3 border-t border-white/10 flex justify-end">
        <button onClick={onSave} className="px-3 py-2 rounded-lg bg-white text-black text-sm font-semibold hover:opacity-90">Save</button>
      </div>
    </div>
  );
}

/* ========================= SERVER PROXY (Example) =========================

Create /api/openrouter route in your app (Next.js API route, Express, etc.)
This protects your OpenRouter API key and allows you to inject/validate the
model + shape messages + append the dataContext as you see fit (RAG/tools).

// Next.js (app router) example: app/api/openrouter/route.ts
import { NextRequest, NextResponse } from "next/server";

export async function POST(req: NextRequest) {
  try {
    const { model, messages, dataContext, temperature = 0.2, maxTokens = 1200 } = await req.json();

    // Validate model against allowlist
    const ALLOWED = new Set(["openrouter/gpt-5", "anthropic/claude-3.5-sonnet", "openai/gpt-4.1", "google/gemini-1.5-pro"]);
    if (!ALLOWED.has(model)) return NextResponse.json({ error: "model_not_allowed" }, { status: 400 });

    // Compose final messages: inject data context as a system addendum for grounding
    const sysIndex = messages.findIndex((m: any) => m.role === "system");
    const baseSystem = sysIndex >= 0 ? messages[sysIndex].content : "";
    const groundedSystem = baseSystem + "\n\nDATA CONTEXT (JSON):\n" + JSON.stringify(dataContext).slice(0, 250_000);

    const finalMessages = [
      { role: "system", content: groundedSystem },
      ...messages.filter((m: any, i: number) => i !== sysIndex),
    ];

    // Call OpenRouter
    const OR_API_KEY = process.env.OPENROUTER_API_KEY as string;
    if (!OR_API_KEY) return NextResponse.json({ error: "missing_api_key" }, { status: 500 });

    const resp = await fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${OR_API_KEY}`,
        "HTTP-Referer": process.env.SITE_URL || "http://localhost:3000",
        "X-Title": "Bristol Analyst",
      },
      body: JSON.stringify({
        model,
        messages: finalMessages,
        temperature,
        max_tokens: maxTokens,
      }),
    });

    if (!resp.ok) {
      const txt = await resp.text();
      return NextResponse.json({ error: "openrouter_error", details: txt }, { status: 502 });
    }

    const json = await resp.json();
    const text = json?.choices?.[0]?.message?.content || "";
    return NextResponse.json({ text });
  } catch (err: any) {
    return NextResponse.json({ error: err?.message || "unknown" }, { status: 500 });
  }
}

// Express example (server.ts)
import express from "express";
import fetch from "node-fetch";
const app = express();
app.use(express.json({ limit: "2mb" }));
app.post("/api/openrouter", async (req, res) => {
  try {
    const { model, messages, dataContext, temperature = 0.2, maxTokens = 1200 } = req.body || {};
    const allow = new Set(["openrouter/gpt-5", "anthropic/claude-3.5-sonnet", "openai/gpt-4.1", "google/gemini-1.5-pro"]);
    if (!allow.has(model)) return res.status(400).json({ error: "model_not_allowed" });

    const sys = messages.find((m: any) => m.role === "system")?.content || "";
    const grounded = sys + "\n\nDATA CONTEXT (JSON):\n" + JSON.stringify(dataContext).slice(0, 250_000);
    const final = [{ role: "system", content: grounded }, ...messages.filter((m: any) => m.role !== "system")];

    const r = await fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`,
        "HTTP-Referer": process.env.SITE_URL || "http://localhost:3000",
        "X-Title": "Bristol Analyst",
      },
      body: JSON.stringify({ model, messages: final, temperature, max_tokens: maxTokens }),
    });

    if (!r.ok) return res.status(502).json({ error: "openrouter_error", details: await r.text() });

    const j = await r.json();
    const text = j?.choices?.[0]?.message?.content || "";
    res.json({ text });
  } catch (e: any) {
    res.status(500).json({ error: e?.message || "unknown" });
  }
});

app.listen(3000, () => console.log("Proxy listening on 3000"));

======================================================================== */
