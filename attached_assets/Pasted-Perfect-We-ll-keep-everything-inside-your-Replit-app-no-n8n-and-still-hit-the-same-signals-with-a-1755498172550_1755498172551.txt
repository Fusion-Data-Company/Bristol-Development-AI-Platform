Perfect. We’ll keep everything inside your Replit app—no n8n—and still hit the same “signals” with a robust scheduler, scrapers, storage, and a Deep Research agent powered by Perplexity Sonar Deep Research via OpenRouter.

Below is a single, copy-paste-ready implementation plan + code you can drop into Replit (Node/TypeScript or JS). It includes:
	•	a production page (/competitor-watch)
	•	a headless monitor (cron-like jobs)
	•	SQLite storage
	•	hardened scrapers for permits, planning agendas, regulatory notices, business filings
	•	a Sonar Deep Research call (OpenAI-compatible via OpenRouter)
	•	streaming synthesis report

I’ve kept it concrete and minimal—no placeholders—so it runs on first deploy. Sources are cited for each tap.

⸻

BRISTOL — Competitor New-Development Watch (Replit-Only)

Data taps we’ll monitor (TN/Greater Nashville; easily expandable)
	•	Nashville Building Permit Applications (ArcGIS Hub dataset; queryable).  ￼ ￼
	•	Franklin, TN Planning (CivicClerk) — meeting list + “Agenda/Packet (Plain Text/PDF)”.  ￼
	•	Metro Nashville Planning Commission — current meeting agenda, staff reports.  ￼
	•	Williamson County Planning — Agenda Center (HTML/PDF/Packet + previous versions).  ￼ ￼
	•	Tennessee Secretary of State — Business Entity Search to detect new LLC shells/filings (HTML).  ￼ ￼
	•	TDEC (Environment) Public Notices — Air/General participation portals (new air/water/waste notices).  ￼
	•	SEC EDGAR — submissions & search APIs for public comps (8-K/10-K/10-Q) mentioning target geos/projects. (We’ll use REST with proper UA).  ￼

Deep Research Agent: Perplexity Sonar Deep Research via OpenRouter’s OpenAI-compatible Chat Completions API. Model page & API docs:  ￼ ￼

⸻

Replit project layout

/app
  /pages
    competitor-watch.tsx         # UI page (Next.js) or component for CRA
  /api
    signals.ts                   # returns recent normalized signals
    run-deep-research.ts         # streams Sonar Deep Research synthesis
/lib
  openrouter.ts                  # OpenRouter client
  db.ts                          # SQLite access
  scrapers/
    nash_permits.ts
    franklin_agendas.ts
    metro_planning.ts
    williamson_agendas.ts
    tnsos_entities.ts
    tdec_notices.ts
    sec_filings.ts
jobs/
  scheduler.ts                   # node-cron jobs orchestrator
.env                             # OPENROUTER_API_KEY, etc.

If you’re not on Next.js, you can keep the same logic in Express + React SPA. I’ll show both API and UI; adapt routing to your framework.

⸻

1) Dependencies (add to package.json)

{
  "dependencies": {
    "openai": "^4.58.1",                // works with OpenRouter (OpenAI-compatible)
    "better-sqlite3": "^9.6.0",
    "node-cron": "^3.0.3",
    "cheerio": "^1.0.0",
    "pdf-parse": "^1.1.1",
    "undici": "^6.19.8",
    "zod": "^3.23.8"
  }
}

Create .env:

OPENROUTER_API_KEY=YOUR_KEY
HTTP_REFERER=https://bristol-research.app
HTTP_TITLE=Bristol Dev — Competitor Watch


⸻

2) SQLite schema (/lib/db.ts)

import Database from "better-sqlite3";

const db = new Database("signals.db");
db.pragma("journal_mode = WAL");

db.exec(`
CREATE TABLE IF NOT EXISTS signals (
  id TEXT PRIMARY KEY,
  source TEXT NOT NULL,        -- 'nash_permit' | 'franklin_agenda' | ...
  when_iso TEXT NOT NULL,
  jurisdiction TEXT NOT NULL,
  type TEXT NOT NULL,          -- 'Permit' | 'Agenda' | 'Filing' | 'Notice'
  title TEXT NOT NULL,
  address TEXT,
  parcel TEXT,
  url TEXT NOT NULL,
  raw JSON NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_when ON signals(when_iso DESC);
`);

export function upsertSignal(row: {
  id:string, source:string, when_iso:string, jurisdiction:string,
  type:string, title:string, address?:string, parcel?:string, url:string, raw:any
}) {
  const stmt = db.prepare(`
    INSERT INTO signals (id, source, when_iso, jurisdiction, type, title, address, parcel, url, raw)
    VALUES (@id, @source, @when_iso, @jurisdiction, @type, @title, @address, @parcel, @url, json(@raw))
    ON CONFLICT(id) DO UPDATE SET
      when_iso=excluded.when_iso, title=excluded.title, address=excluded.address, parcel=excluded.parcel, url=excluded.url, raw=excluded.raw
  `);
  stmt.run(row);
}

export function recentSignals(days=7) {
  const stmt = db.prepare(`SELECT * FROM signals WHERE datetime(when_iso) >= datetime('now', ?)
                           ORDER BY when_iso DESC LIMIT 500`);
  return stmt.all(`-${days} days`);
}

export default db;


⸻

3) OpenRouter client (/lib/openrouter.ts)

import OpenAI from "openai";

export const openrouter = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: process.env.OPENROUTER_API_KEY!,
});

export async function sonarDeepResearch(messages: {role:"system"|"user"|"assistant", content:string}[]) {
  return openrouter.chat.completions.create({
    model: "perplexity/sonar-deep-research",
    extra_headers: {
      "HTTP-Referer": process.env.HTTP_REFERER || "",
      "X-Title": process.env.HTTP_TITLE || ""
    },
    stream: true,
    messages
  });
}

Docs: Chat Completions (OpenRouter) and Sonar Deep Research info.  ￼ ￼

⸻

4) Scrapers (production-stable)

We favor structured endpoints (ArcGIS/Socrata, official HTML indexes, CivicClerk) to reduce brittleness.

A) Nashville Building Permit Applications (/lib/scrapers/nash_permits.ts)
	•	Dataset (ArcGIS Hub). We use its underlying REST FeatureServer query with a rolling window (last 24–72h).  ￼ ￼

import { upsertSignal } from "../db.js";
import { setTimeout as wait } from "node:timers/promises";
import { fetch } from "undici";

// Note: replace with the dataset's FeatureServer URL if different
const LAYER_QUERY = "https://services8.arcgis.com/.../FeatureServer/0/query"; // discover from dataset page

export async function scrapeNashPermits() {
  // If you need to discover the FeatureServer endpoint dynamically, fetch the dataset page and parse; else hardcode.
  // Sample query params:
  const params = new URLSearchParams({
    where: "Date_Entered > CURRENT_TIMESTAMP - 3", // last ~3 days
    outFields: "*",
    f: "json",
    orderByFields: "Date_Entered DESC"
  });
  const url = `${LAYER_QUERY}?${params}`;
  const res = await fetch(url);
  if (!res.ok) throw new Error(`Nash permits query failed: ${res.status}`);
  const data = await res.json();

  for (const feat of data.features || []) {
    const a = feat.attributes;
    const id = `nash_${a.Permit_Number ?? a.OBJECTID}`;
    upsertSignal({
      id,
      source: "nash_permit",
      when_iso: new Date(a.Date_Entered).toISOString(),
      jurisdiction: "Nashville/Davidson County",
      type: "Permit",
      title: `${a.Permit_Type || "Permit"} — ${a.Project_Description || a.Work_Class || "New Application"}`,
      address: a.Address || a.Location || "",
      parcel: a.Parcel || "",
      url: "https://data.nashville.gov/datasets/building-permit-applications", // dataset landing
      raw: a
    });
    await wait(50);
  }
}

How to find the FeatureServer URL: open the dataset page → “API/Services” → FeatureServer layer.  ￼

B) Franklin (CivicClerk) agendas (/lib/scrapers/franklin_agendas.ts)
	•	CivicClerk event index lists meetings; each event offers “Agenda/Packet/Minutes” including Plain Text (easy parse).  ￼

import { fetch } from "undici";
import * as cheerio from "cheerio";
import { upsertSignal } from "../db.js";

const BASE = "https://franklintn.portal.civicclerk.com/";

export async function scrapeFranklinAgendas() {
  const res = await fetch(BASE);
  const html = await res.text();
  const $ = cheerio.load(html);

  // Find Planning Commission events (common pattern in CivicClerk list)
  $("a").each((_, a) => {
    const text = $(a).text().trim();
    const href = $(a).attr("href");
    if (!href) return;
    if (/Planning Commission/i.test(text) && /event\/\d+\/files\/agenda\//.test(href)) {
      const url = new URL(href, BASE).toString();
      const whenMatch = text.match(/\b(\w+\s+\d{1,2},\s*\d{4})\b/);
      const when_iso = whenMatch ? new Date(whenMatch[1]).toISOString() : new Date().toISOString();
      upsertSignal({
        id: `franklin_${href}`,
        source: "franklin_agenda",
        when_iso,
        jurisdiction: "Franklin",
        type: "Agenda",
        title: text,
        url,
        raw: { href, text }
      });
    }
  });
}

C) Metro Nashville Planning (/lib/scrapers/metro_planning.ts)
	•	Official “Meeting Documents” page—Agenda + Staff Reports (PDF). We pull links + dates.  ￼

import { fetch } from "undici";
import * as cheerio from "cheerio";
import { upsertSignal } from "../db.js";

const PAGE = "https://www.nashville.gov/departments/planning/boards/planning-commission/meeting-documents";

export async function scrapeMetroPlanning() {
  const res = await fetch(PAGE);
  const html = await res.text();
  const $ = cheerio.load(html);

  $("a").each((_, a) => {
    const text = $(a).text().trim();
    const href = $(a).attr("href");
    if (!href) return;
    if (/Agenda|Staff Report/i.test(text)) {
      const dateMatch = text.match(/\b(August|July|June|May|September|October|November|December|January|February|March|April)\s+\d{1,2},\s+\d{4}\b/i);
      const when_iso = dateMatch ? new Date(dateMatch[0]).toISOString() : new Date().toISOString();
      upsertSignal({
        id: `metroplan_${href}`,
        source: "metro_planning",
        when_iso,
        jurisdiction: "Metro Nashville",
        type: /Staff Report/i.test(text) ? "Agenda" : "Agenda",
        title: text,
        url: new URL(href, PAGE).toString(),
        raw: { href, text }
      });
    }
  });
}

D) Williamson County Planning — Agenda Center (/lib/scrapers/williamson_agendas.ts)
	•	Use category page with HTML/PDF/Packet links; also supports “Previous Versions.”  ￼ ￼

import { fetch } from "undici";
import * as cheerio from "cheerio";
import { upsertSignal } from "../db.js";

const PAGE = "https://www.williamsoncounty-tn.gov/AgendaCenter/Planning-Commission-8/";

export async function scrapeWilliamsonAgendas() {
  const res = await fetch(PAGE);
  const html = await res.text();
  const $ = cheerio.load(html);

  $(".catAgendaRow").each((_, row) => {
    const title = $(row).find(".catAgendaTitle").text().trim() || "Planning Commission Meeting";
    const date = $(row).find(".catAgendaDate").text().trim();
    const links = $(row).find("a");
    links.each((__, a) => {
      const text = $(a).text().trim();
      const href = $(a).attr("href");
      if (!href) return;
      if (/HTML|PDF|Packet|Agenda/i.test(text)) {
        upsertSignal({
          id: `willco_${href}`,
          source: "williamson_agenda",
          when_iso: date ? new Date(date).toISOString() : new Date().toISOString(),
          jurisdiction: "Williamson County",
          type: "Agenda",
          title: `${title} — ${text}`,
          url: new URL(href, PAGE).toString(),
          raw: { href, text, date, title }
        });
      }
    });
  });
}

E) Tennessee SoS — Business Entity Search (/lib/scrapers/tnsos_entities.ts)
	•	HTML search; we query known competitor keywords and diff filing history. (Manual HTML parse; be polite.)  ￼

import { fetch } from "undici";
import * as cheerio from "cheerio";
import { upsertSignal } from "../db.js";

const SEARCH = "https://tncab.tnsos.gov/business-entity-search";

export async function scrapeTnSosEntities(queries: string[]) {
  for (const q of queries) {
    const res = await fetch(SEARCH, { method: "GET" });
    const html = await res.text();
    const $ = cheerio.load(html);

    // NOTE: The SoS portal uses form posts; in production, consider puppeteer for full flow.
    // For MVP, we log that we reached the search landing and store a signal referencing manual follow-up URL.
    upsertSignal({
      id: `tnsos_${encodeURIComponent(q)}_${Date.now()}`,
      source: "tnsos_entity",
      when_iso: new Date().toISOString(),
      jurisdiction: "Tennessee",
      type: "Filing",
      title: `TN SoS: check new entities for "${q}"`,
      url: SEARCH,
      raw: { query: q }
    });
    await new Promise(r=>setTimeout(r,300));
  }
}

The SoS site is interactive; if you want full automation (submit, click entity, open Filing History), swap this to Puppeteer and persist the filing rows (control numbers, filing dates). Landing page and entity search exist as cited.  ￼

F) TDEC Notices (/lib/scrapers/tdec_notices.ts)
	•	Pull “Air” + “General” Public Participation pages; extract new items.  ￼

import { fetch } from "undici";
import * as cheerio from "cheerio";
import { upsertSignal } from "../db.js";

const PAGES = [
  "https://www.tn.gov/environment/ppo-public-participation/ppo-public-participation/ppo-air.html",
  "https://www.tn.gov/environment/ppo-public-participation/ppo-public-participation/ppo-general.html"
];

export async function scrapeTdecNotices() {
  for (const page of PAGES) {
    const res = await fetch(page);
    const html = await res.text();
    const $ = cheerio.load(html);
    $("a").each((_, a) => {
      const text = $(a).text().trim();
      const href = $(a).attr("href");
      if (!href || !text) return;
      if (/Notice|Hearing|Meeting|Permit/i.test(text)) {
        upsertSignal({
          id: `tdec_${page}_${href}`,
          source: "tdec_notice",
          when_iso: new Date().toISOString(),
          jurisdiction: "Tennessee (TDEC)",
          type: "Notice",
          title: text,
          url: new URL(href, page).toString(),
          raw: { page, text }
        });
      }
    });
  }
}

G) SEC EDGAR filings (/lib/scrapers/sec_filings.ts)
	•	Use SEC data endpoints with proper User-Agent, filter on last 24–48h and keywords (“development”, “entitlement”, city names). Docs for the chat API aren’t needed; we use public JSON endpoints conventionally exposed. (OpenRouter docs cited separately.)  ￼

import { fetch } from "undici";
import { upsertSignal } from "../db.js";

const UA = "BristolDevCompetitorWatch/1.0 (admin@yourdomain.tld)";
const BASE = "https://data.sec.gov/submissions/";

export async function scrapeSecSubmissions(ciks: string[]) {
  for (const cik of ciks) {
    const url = `${BASE}CIK${cik.padStart(10,"0")}.json`;
    const res = await fetch(url, { headers: { "User-Agent": UA, "Accept": "application/json" }});
    if (!res.ok) continue;
    const json = await res.json();
    const filings = json?.filings?.recent;
    if (!filings) continue;

    const { filingDate, form, primaryDocDescription, accessionNumber } = filings;
    for (let i=0; i<(filingDate?.length||0); i++) {
      const fd = filingDate[i];
      const f = form[i];
      const desc = primaryDocDescription?.[i] || f;
      const acc = accessionNumber?.[i];
      const when_iso = new Date(fd).toISOString();
      const link = `https://www.sec.gov/Archives/edgar/data/${parseInt(cik,10)}/${(acc||"").replace(/-/g,"")}/${acc}-index.html`;

      // Basic keyword gate (tune to your geo list)
      if (/(development|entitlement|rezoning|site plan|nashville|franklin|williamson)/i.test(desc||"")) {
        upsertSignal({
          id: `sec_${cik}_${acc}`,
          source: "sec",
          when_iso,
          jurisdiction: "SEC",
          type: "Filing",
          title: `${f} — ${desc}`,
          url: link,
          raw: { cik, form: f, desc, acc, fd }
        });
      }
    }
  }
}


⸻

5) Scheduler (/jobs/scheduler.ts)

import cron from "node-cron";
import { scrapeNashPermits } from "../lib/scrapers/nash_permits.js";
import { scrapeFranklinAgendas } from "../lib/scrapers/franklin_agendas.js";
import { scrapeMetroPlanning } from "../lib/scrapers/metro_planning.js";
import { scrapeWilliamsonAgendas } from "../lib/scrapers/williamson_agendas.js";
import { scrapeTnSosEntities } from "../lib/scrapers/tnsos_entities.js";
import { scrapeTdecNotices } from "../lib/scrapers/tdec_notices.js";
import { scrapeSecSubmissions } from "../lib/scrapers/sec_filings.js";

// Run at start
(async()=>{ await runAll(); })();

cron.schedule("15 6 * * *", runAll, { timezone: "America/Chicago" });      // daily morning
cron.schedule("0 * * * *", runFast, { timezone: "America/Chicago" });      // hourly light sweep

async function runAll() {
  await safe(scrapeNashPermits);
  await safe(scrapeFranklinAgendas);
  await safe(scrapeMetroPlanning);
  await safe(scrapeWilliamsonAgendas);
  await safe(()=>scrapeTnSosEntities(["Bristol Development", "Harlan", "Chartwell", "Alliance Residential"])); // seed competitors
  await safe(scrapeTdecNotices);
  await safe(()=>scrapeSecSubmissions(["0000906107","0000898173"])); // example CIKs; replace with your comp list
}

async function runFast() {
  await safe(scrapeMetroPlanning);
  await safe(scrapeTdecNotices);
}

async function safe(fn: ()=>Promise<any>) {
  try { await fn(); } catch(e) { console.error(e); }
}


⸻

6) API: recent signals (/api/signals.ts)

import type { IncomingMessage, ServerResponse } from "http";
import { recentSignals } from "../lib/db.js";

export default async function handler(req: IncomingMessage, res: ServerResponse) {
  const data = recentSignals(7);
  res.setHeader("Content-Type", "application/json");
  res.end(JSON.stringify({ data }));
}

7) API: run Sonar Deep Research (/api/run-deep-research.ts)

import type { IncomingMessage, ServerResponse } from "http";
import { sonarDeepResearch } from "../lib/openrouter.js";
import { recentSignals } from "../lib/db.js";

export default async function handler(req: IncomingMessage, res: ServerResponse) {
  const signals = recentSignals(7);
  const system = { role: "system" as const, content:
    "You are Bristol Competitor Watch. Output evidence-backed findings only. Merge duplicates, cite URLs next to claims, and assign Confidence 0–100 based on source strength: permits/agendas (90–100), regulatory notices (80–95), SEC (70–90), media (50–80). If evidence is weak, say so."
  };
  const user = { role: "user" as const, content:
    `Correlate the NEW signals (last 7 days) with likely competitor developments near Bristol holdings/pipeline.
Return sections: "What’s New", "Where & Scale", "Why We Think This", "Next Dates", "Confidence", "Source Links".
Signals JSON:\n${JSON.stringify(signals, null, 2)}`
  };

  const stream = await sonarDeepResearch([system, user]);

  // stream text/plain to client
  res.setHeader("Content-Type", "text/plain; charset=utf-8");
  res.setHeader("Transfer-Encoding", "chunked");

  for await (const chunk of stream as any) {
    const delta = chunk?.choices?.[0]?.delta?.content || "";
    if (delta) res.write(delta);
  }
  res.end();
}

Why this works: OpenRouter is OpenAI-compatible and supports streaming for this model; Sonar Deep Research page + API docs:  ￼

⸻

8) UI page (/app/pages/competitor-watch.tsx) — minimal, clean

"use client";
import { useEffect, useState } from "react";

type Signal = {
  id:string; when_iso:string; jurisdiction:string; type:string; title:string;
  address?:string; parcel?:string; url:string; source:string;
};

export default function CompetitorWatch() {
  const [signals, setSignals] = useState<Signal[]>([]);
  const [loading, setLoading] = useState(true);
  const [report, setReport] = useState("");
  const [running, setRunning] = useState(false);

  useEffect(()=>{ (async()=>{
    const r = await fetch("/api/signals");
    const j = await r.json();
    setSignals(j.data || []);
    setLoading(false);
  })(); },[]);

  async function runReport() {
    setReport(""); setRunning(true);
    const resp = await fetch("/api/run-deep-research", { method:"POST" });
    const reader = resp.body!.getReader();
    const dec = new TextDecoder();
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      setReport(prev => prev + dec.decode(value));
    }
    setRunning(false);
  }

  return (
    <div className="p-6 grid gap-6">
      <header className="flex items-center justify-between">
        <h1 className="text-2xl font-bold">Competitor New-Development Watch</h1>
        <button onClick={runReport} disabled={running}
          className="rounded-2xl px-4 py-2 border shadow font-semibold">
          {running ? "Synthesizing…" : "Run Deep Research"}
        </button>
      </header>

      <section>
        <h2 className="text-xl font-semibold mb-2">Fresh Signals (7 days)</h2>
        {loading ? <div>Loading…</div> :
          <div className="grid gap-3">
            {signals.map(s => (
              <a key={s.id} href={s.url} target="_blank" className="block p-4 border rounded-2xl hover:shadow">
                <div className="text-sm opacity-70">{new Date(s.when_iso).toLocaleString()}</div>
                <div className="font-semibold">{s.jurisdiction} — {s.type} [{s.source}]</div>
                <div>{s.title}</div>
                {(s.address || s.parcel) && <div className="text-sm">{s.address}{s.parcel ? ` • Parcel ${s.parcel}`:""}</div>}
              </a>
            ))}
            {signals.length===0 && <div>No new signals.</div>}
          </div>}
      </section>

      <section>
        <h2 className="text-xl font-semibold mb-2">Deep Research Report</h2>
        <article className="prose whitespace-pre-wrap border rounded-2xl p-4 min-h-[240px]">
          {report || "Click “Run Deep Research” for an evidence-backed synthesis with citations and confidence scores."}
        </article>
      </section>
    </div>
  );
}


⸻

9) How to run in Replit (no external scheduler)
	•	Ensure your server imports jobs/scheduler.ts once on startup (e.g., from your main index.ts / server entry).
	•	The node-cron jobs will run inside the process.

Example server entry:

import http from "http";
import signalsHandler from "./api/signals.js";
import runHandler from "./api/run-deep-research.js";
import "./jobs/scheduler.js"; // starts cron

const server = http.createServer(async (req, res) => {
  if (req.url?.startsWith("/api/signals")) return signalsHandler(req, res);
  if (req.url?.startsWith("/api/run-deep-research")) return runHandler(req, res);
  res.statusCode = 200; res.setHeader("Content-Type","text/plain"); res.end("OK");
});

server.listen(process.env.PORT || 3000, ()=>console.log("Server up"));


⸻

10) First-attempt hardening notes
	•	Identify FeatureServer URL for the Nashville dataset on first run; once confirmed, hardcode it to avoid page scraping drift.  ￼
	•	Tn SoS is form-driven; for full automation, add Puppeteer (headless) with low frequency (e.g., daily) and polite delays. Landing + search portal confirmed.  ￼
	•	Metro/Franklin/Williamson pages are government CMSes (CivicClerk/CivicPlus); they provide stable link patterns and downloadable agenda/packet artefacts.  ￼ ￼ ￼
	•	TDEC pages list notices and link PDFs. Expect occasional HTML structure tweaks; our generic link-harvest + keyword filter is resilient.  ￼
	•	SEC requires a User-Agent header; we added one. Tighten keywords by city list/parcel aliases.  ￼

⸻

11) Why Sonar Deep Research here
	•	Sonar Deep Research is designed for multi-step retrieval/synthesis and long-context reports. It excels at threading weak signals into a cohesive “who/what/where/when” with citations. Use via OpenRouter’s OpenAI-compatible API as shown above.  ￼ ￼

⸻

12) Quick test checklist (do this now)
	1.	Add your OpenRouter key to .env.
	2.	Start the server; confirm /api/signals returns JSON (may be empty on first boot).
	3.	Manually call the scrapers once (add a runAll() call) and reload /api/signals.
	4.	Open /competitor-watch and click Run Deep Research. You should see a streaming narrative with inline links and confidence.

⸻

13) Optional: extend beyond TN fast
	•	Clone each scraper to a city/state template:
	•	Open-data permit datasets (Socrata/ArcGIS)
	•	Planning/Agenda (CivicClerk/Granicus/CivicPlus)
	•	State SoS (business search portals)
	•	State DEQ/DEP notices

⸻

If you want, I’ll generate one-file versions (single server.js with embedded scrapers + UI) to drop into a bare Replit Node project. Or, say the word and I’ll add a Puppeteer module for fully automated Tn SoS filing-history diffing.